--- load options ---
batch_size: 2
concat: 1
crop_size: 128
d_iter: 3
dataroot: ../datasets/summer2winter_yosemite
decay_temp: 1
decay_temp_rate: 0.013862944
dis_norm: None
dis_scale: 3
dis_spectral_norm: False
display_dir: ../logs
display_freq: 1
gaussian_size: 64
gpu: 0
hard_gumbel: 0
img_save_freq: 5
init_temp: 1.0
input_dim_a: 3
input_dim_b: 3
lr_policy: lambda
min_temp: 0.5
model_save_freq: 10
nThreads: 8
n_ep: 1200
n_ep_decay: 600
name: yosemite
no_display_img: False
no_flip: False
no_ms: False
num_classes: 2
phase: train
resize_size: 148
result_dir: ../results
resume: None
x_dim: 262144

--- load dataset ---
A: 1231, B: 962 images

--- load model ---
x_dim 262144 64 2
x_dim 262144 64 2
x_dim 64 2 64
start the training at epoch 0

--- train ---
size image_a torch.Size([2, 3, 128, 128])
Entra in if train
ca torch.Size([1, 3, 128, 128])
size of outputA1:  torch.Size([1, 256, 32, 32])
size of outputA:  torch.Size([1, 256, 32, 32])
Size of flatten_A:  torch.Size([1, 262144])
size of flatten_B  torch.Size([1, 262144])
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=262144, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=262144, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
infernce_outputA:  {'mean': tensor([[0.0510, 0.3481]], device='cuda:0', grad_fn=<AddmmBackward0>), 'var': tensor([[0.7846, 0.9842]], device='cuda:0', grad_fn=<SoftplusBackward0>), 'gaussian': tensor([[-0.6032, -0.4735]], device='cuda:0', grad_fn=<AddBackward0>), 'logits': tensor([[-0.2646, -0.2089, -0.2588, -0.2816,  0.1756,  0.2313,  0.1777, -0.1313,
          0.2383,  0.3115, -0.1788,  0.2521, -0.3625, -0.0427, -0.1095,  0.1069,
         -0.2368,  0.0943,  0.1745, -0.1368, -0.0470,  0.0593,  0.0658, -0.4289,
         -0.1465,  0.5292,  0.0188, -0.0981, -0.2265,  0.4342,  0.1141,  0.1061,
         -0.2052, -0.1896,  0.0848, -0.0838, -0.2451,  0.3002,  0.0700, -0.1059,
         -0.2386,  0.4452,  0.3720,  0.0015,  0.1999, -0.1120,  0.1881, -0.0319,
         -0.1165, -0.4385,  0.0326,  0.0224, -0.0378, -0.0310, -0.2069, -0.2038,
         -0.0607,  0.0550,  0.3121, -0.0979,  0.2639, -0.0360,  0.1572,  0.1145]],
       device='cuda:0', grad_fn=<ViewBackward0>), 'prob_cat': tensor([[0.0117, 0.0124, 0.0118, 0.0115, 0.0182, 0.0192, 0.0182, 0.0134, 0.0193,
         0.0208, 0.0127, 0.0196, 0.0106, 0.0146, 0.0137, 0.0170, 0.0120, 0.0167,
         0.0181, 0.0133, 0.0145, 0.0162, 0.0163, 0.0099, 0.0132, 0.0259, 0.0155,
         0.0138, 0.0121, 0.0235, 0.0171, 0.0169, 0.0124, 0.0126, 0.0166, 0.0140,
         0.0119, 0.0206, 0.0163, 0.0137, 0.0120, 0.0238, 0.0221, 0.0153, 0.0186,
         0.0136, 0.0184, 0.0148, 0.0136, 0.0098, 0.0157, 0.0156, 0.0147, 0.0148,
         0.0124, 0.0124, 0.0143, 0.0161, 0.0208, 0.0138, 0.0198, 0.0147, 0.0178,
         0.0171]], device='cuda:0', grad_fn=<SoftmaxBackward0>), 'categorical': tensor([[3.8996e-04, 1.5671e-03, 1.3162e-03, 6.4023e-02, 4.6783e-04, 2.2984e-03,
         1.1163e-02, 5.7906e-04, 4.3726e-03, 5.3405e-03, 3.6812e-04, 1.7171e-03,
         3.8436e-03, 1.5537e-03, 3.5757e-04, 1.0557e-03, 3.9208e-04, 3.4827e-04,
         7.8945e-04, 2.0890e-03, 3.9306e-02, 5.6401e-03, 5.4087e-04, 7.6326e-04,
         1.2551e-03, 6.4579e-01, 1.6471e-03, 9.5039e-04, 2.4858e-03, 1.4976e-03,
         1.6553e-03, 4.7354e-04, 9.0964e-04, 1.0388e-02, 1.8292e-03, 9.2784e-04,
         1.6157e-03, 3.2282e-02, 2.3586e-03, 1.4608e-03, 7.1780e-04, 1.4481e-02,
         9.8256e-03, 3.6699e-03, 9.6182e-04, 3.3968e-03, 4.2494e-04, 5.5055e-03,
         7.3041e-04, 1.4425e-02, 1.1088e-02, 9.0571e-04, 9.4971e-04, 1.4702e-03,
         5.6422e-04, 2.3393e-03, 2.7006e-04, 1.8075e-03, 1.3833e-03, 2.0056e-04,
         4.2782e-03, 5.7604e-02, 1.0430e-03, 4.1527e-03]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)}
Esce da rward E_content
forward ok
forward_content ok
imageA tensor([[-0.6032, -0.4735]], device='cuda:0', grad_fn=<AddBackward0>)
imageB tensor([[0.5239, 0.7380]], device='cuda:0', grad_fn=<AddBackward0>)
x tensor([[-0.6032, -0.4735]], device='cuda:0')
Entra in dim(x)=2
x: torch.Size([1, 1, 1, 2])
x tensor([[0.5239, 0.7380]], device='cuda:0')
Entra in dim(x)=2
x: torch.Size([1, 1, 1, 2])
size image_a torch.Size([2, 3, 128, 128])
Entra in if train
ca torch.Size([1, 3, 128, 128])
size of outputA1:  torch.Size([1, 256, 32, 32])
size of outputA:  torch.Size([1, 256, 32, 32])
Size of flatten_A:  torch.Size([1, 262144])
size of flatten_B  torch.Size([1, 262144])
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=262144, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=262144, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
infernce_outputA:  {'mean': tensor([[-0.0606,  0.1096]], device='cuda:0', grad_fn=<AddmmBackward0>), 'var': tensor([[0.8024, 0.9054]], device='cuda:0', grad_fn=<SoftplusBackward0>), 'gaussian': tensor([[1.5573, 0.4319]], device='cuda:0', grad_fn=<AddBackward0>), 'logits': tensor([[-0.1768,  0.0281, -0.4026, -0.1100,  0.1095, -0.0100, -0.0010,  0.0808,
          0.0666,  0.3430, -0.3330,  0.2002, -0.0561,  0.0915,  0.1384,  0.3904,
          0.0167,  0.1158,  0.2984,  0.0404, -0.0542,  0.3879, -0.0428,  0.0357,
         -0.1757,  0.2724,  0.0363,  0.0327, -0.1889,  0.1503, -0.2840, -0.2662,
         -0.2743, -0.0793,  0.0322,  0.1583, -0.0711,  0.2241, -0.1369,  0.1532,
         -0.0026,  0.2101,  0.3683,  0.2428, -0.0766, -0.1594,  0.1792, -0.2751,
          0.3423, -0.4980, -0.7372,  0.2185,  0.1423, -0.1109, -0.2148, -0.1368,
          0.0251,  0.1951,  0.3453,  0.0470,  0.0051,  0.0539, -0.1426, -0.3469]],
       device='cuda:0', grad_fn=<ViewBackward0>), 'prob_cat': tensor([[0.0127, 0.0156, 0.0101, 0.0136, 0.0169, 0.0150, 0.0151, 0.0164, 0.0162,
         0.0214, 0.0109, 0.0185, 0.0143, 0.0166, 0.0174, 0.0224, 0.0154, 0.0170,
         0.0204, 0.0158, 0.0144, 0.0223, 0.0145, 0.0157, 0.0127, 0.0199, 0.0157,
         0.0157, 0.0125, 0.0176, 0.0114, 0.0116, 0.0115, 0.0140, 0.0157, 0.0178,
         0.0141, 0.0190, 0.0132, 0.0177, 0.0151, 0.0187, 0.0219, 0.0193, 0.0140,
         0.0129, 0.0181, 0.0115, 0.0213, 0.0092, 0.0073, 0.0189, 0.0175, 0.0136,
         0.0122, 0.0132, 0.0155, 0.0184, 0.0214, 0.0159, 0.0152, 0.0160, 0.0131,
         0.0107]], device='cuda:0', grad_fn=<SoftmaxBackward0>), 'categorical': tensor([[0.0051, 0.0050, 0.0049, 0.0021, 0.0076, 0.0065, 0.0031, 0.0026, 0.0097,
         0.0017, 0.0016, 0.0082, 0.0028, 0.0030, 0.1362, 0.0239, 0.0014, 0.0129,
         0.0442, 0.0036, 0.0030, 0.0022, 0.0005, 0.0781, 0.0024, 0.0113, 0.0057,
         0.0068, 0.0023, 0.0018, 0.0019, 0.0013, 0.0008, 0.0071, 0.0061, 0.0045,
         0.0068, 0.0592, 0.0025, 0.0029, 0.0026, 0.0309, 0.0041, 0.0489, 0.0051,
         0.1407, 0.0015, 0.0014, 0.0046, 0.0247, 0.0072, 0.0105, 0.0029, 0.0020,
         0.0159, 0.0126, 0.1317, 0.0074, 0.0100, 0.0014, 0.0269, 0.0099, 0.0028,
         0.0012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)}
Esce da rward E_content
forward ok
forward_content ok
imageA tensor([[1.5573, 0.4319]], device='cuda:0', grad_fn=<AddBackward0>)
imageB tensor([[-0.5577, -0.5719]], device='cuda:0', grad_fn=<AddBackward0>)
x tensor([[1.5573, 0.4319]], device='cuda:0')
Entra in dim(x)=2
x: torch.Size([1, 1, 1, 2])
x tensor([[-0.5577, -0.5719]], device='cuda:0')
Entra in dim(x)=2
x: torch.Size([1, 1, 1, 2])
size image_a torch.Size([2, 3, 128, 128])
Entra in else train
ca torch.Size([1, 3, 128, 128])
size of outputA1:  torch.Size([1, 256, 32, 32])
size of outputA:  torch.Size([1, 256, 32, 32])
Size of flatten_A:  torch.Size([1, 262144])
size of flatten_B  torch.Size([1, 262144])
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=262144, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=262144, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
infernce_outputA:  {'mean': tensor([[-0.1538,  0.1383]], device='cuda:0', grad_fn=<AddmmBackward0>), 'var': tensor([[0.7501, 0.7745]], device='cuda:0', grad_fn=<SoftplusBackward0>), 'gaussian': tensor([[-0.7217, -0.5610]], device='cuda:0', grad_fn=<AddBackward0>), 'logits': tensor([[-0.2495, -0.2679, -0.5240, -0.1312,  0.1765, -0.0580, -0.2439,  0.0198,
          0.4927,  0.2057, -0.2328,  0.3323, -0.3016,  0.1785, -0.0691,  0.2767,
         -0.3906,  0.1267,  0.2826, -0.1044,  0.1763, -0.0234, -0.1312, -0.0317,
         -0.0062,  0.0054,  0.3390,  0.0449, -0.0904,  0.2738,  0.0260, -0.0192,
         -0.0504, -0.1664,  0.0758, -0.0025, -0.0519,  0.5166,  0.1123,  0.0238,
          0.1809,  0.2019,  0.0687, -0.0371,  0.0979, -0.1371,  0.1747, -0.2891,
         -0.2452, -0.2743, -0.0882,  0.0536,  0.1886, -0.0985, -0.3499,  0.0777,
         -0.2977,  0.0761,  0.3007, -0.0846,  0.2257, -0.0670, -0.3545, -0.1664]],
       device='cuda:0', grad_fn=<ViewBackward0>), 'prob_cat': tensor([[0.0120, 0.0117, 0.0091, 0.0135, 0.0183, 0.0145, 0.0120, 0.0156, 0.0251,
         0.0188, 0.0122, 0.0214, 0.0113, 0.0183, 0.0143, 0.0202, 0.0104, 0.0174,
         0.0204, 0.0138, 0.0183, 0.0150, 0.0135, 0.0149, 0.0152, 0.0154, 0.0215,
         0.0160, 0.0140, 0.0202, 0.0157, 0.0150, 0.0146, 0.0130, 0.0165, 0.0153,
         0.0146, 0.0257, 0.0172, 0.0157, 0.0184, 0.0188, 0.0164, 0.0148, 0.0169,
         0.0134, 0.0183, 0.0115, 0.0120, 0.0117, 0.0140, 0.0162, 0.0185, 0.0139,
         0.0108, 0.0166, 0.0114, 0.0166, 0.0207, 0.0141, 0.0192, 0.0143, 0.0108,
         0.0130]], device='cuda:0', grad_fn=<SoftmaxBackward0>), 'categorical': tensor([[1.0164e-04, 9.0838e-05, 4.6613e-05, 1.6113e-02, 3.0726e-04, 4.7385e-04,
         2.9806e-04, 2.4327e-03, 8.2978e-04, 9.1344e-04, 9.7592e-05, 4.6120e-04,
         8.8784e-04, 6.3464e-03, 9.4604e-04, 3.1656e-04, 4.9633e-04, 4.1114e-04,
         4.8165e-04, 2.5868e-04, 7.7018e-04, 7.2017e-04, 3.9887e-04, 2.3862e-04,
         1.8580e-04, 1.5300e-03, 2.1824e-03, 1.4850e-03, 4.8043e-05, 1.2693e-04,
         8.5182e-01, 2.5429e-03, 1.9546e-04, 3.4632e-04, 2.0001e-04, 1.2200e-04,
         9.9420e-04, 1.4030e-03, 4.4552e-03, 1.3040e-04, 5.3296e-04, 4.4878e-04,
         1.0673e-04, 4.1547e-04, 4.6144e-04, 4.5223e-04, 8.3327e-05, 6.3054e-05,
         1.0153e-02, 4.9951e-03, 2.2964e-04, 5.6436e-04, 3.1557e-03, 9.6038e-04,
         3.6015e-04, 1.1928e-04, 2.0537e-04, 1.4886e-02, 1.6923e-03, 5.6659e-02,
         5.1471e-04, 1.1708e-04, 2.9771e-04, 3.2228e-04]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)}
Esce da rward E_content
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Z_CONTENT_B torch.Size([1, 2])
Z_CONTENT_A torch.Size([1, 2])
Dimensione di z_attr_a: torch.Size([1, 64])
Dimensione di z_random: torch.Size([1, 64])
Dimensione di z_random2: torch.Size([1, 64])
Z_ATTR_B torch.Size([1, 64])
Z_ATTR_A torch.Size([1, 64])
y torch.Size([4, 64])
Traceback (most recent call last):
  File "/home/davide/Greta/DRIT/src/train.py", line 84, in <module>
    main()
  File "/home/davide/Greta/DRIT/src/train.py", line 57, in main
    model.update_D(images_a, images_b)
  File "/home/davide/Greta/DRIT/src/model.py", line 274, in update_D
    self.forward()
  File "/home/davide/Greta/DRIT/src/model.py", line 193, in forward
    output_fakeA = self.gen.forward_a(input_content_forA,input_attr_forA)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/src/networks.py", line 502, in forward_a
    out0 = self.dec_share(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/src/networks.py", line 686, in forward
    out = self.model(x)
          ^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [148, 148, 3, 3], expected input[1, 1, 6, 4] to have 148 channels, but got 1 channels instead