{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9209,"status":"ok","timestamp":1714291153808,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"ixlHCXuwjt7T"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","import argparse\n","import random\n","import numpy as np\n","import os\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.utils.data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install torch"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714291153810,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"X-9RmLpLm7-A"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714291153810,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"C86kkRyrwlJ1"},"outputs":[],"source":["from scipy.io import loadmat"]},{"cell_type":"markdown","metadata":{"id":"tMBj9BnsmsT0"},"source":["### Load dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":832,"status":"ok","timestamp":1714291157219,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"8Yj_wQ8YkJQg"},"outputs":[],"source":["parser = argparse.ArgumentParser(description='PyTorch Implementation of DGM Clustering')\n","\n","## Used only in notebooks\n","parser.add_argument('-f', '--file',\n","                    help='Path for input file. First line should contain number of lines to search in')\n","\n","## Dataset\n","parser.add_argument('--dataset', type=str, choices=['mnist'],\n","                    default='mnist', help='dataset (default: mnist)')\n","parser.add_argument('--seed', type=int, default=0, help='random seed (default: 0)')\n","\n","## GPU\n","parser.add_argument('--cuda', type=int, default=1,\n","                    help='use of cuda (default: 1)') # imposta se usare GPU o no, 1 = si, 0 = no\n","parser.add_argument('--gpuID', type=int, default=0,\n","                    help='set gpu id to use (default: 0)') # imposta l'id della GPU da usare\n","\n","## Training\n","parser.add_argument('--epochs', type=int, default=300,\n","                    help='number of total epochs to run (default: 200)') # numero di epoche di addestramento\n","parser.add_argument('--batch_size', default=64, type=int,\n","                    help='mini-batch size (default: 64)') # dimensione del batch di addestramento\n","parser.add_argument('--batch_size_val', default=200, type=int,\n","                    help='mini-batch size of validation (default: 200)') # dimensione del batch di validazione\n","parser.add_argument('--learning_rate', default=1e-3, type=float,\n","                    help='learning rate (default: 0.001)') # tasso di apprendimento, quanto velocemente il modello impara\n","parser.add_argument('--decay_epoch', default=-1, type=int,\n","                    help='Reduces the learning rate every decay_epoch') # riduce il tasso di apprendimento ad ogni decay_epoch\n","parser.add_argument('--lr_decay', default=0.5, type=float,\n","                    help='Learning rate decay for training (default: 0.5)') # tasso di decadimento del tasso di apprendimento\n","\n","## Architecture\n","parser.add_argument('--num_classes', type=int, default=10,\n","                    help='number of classes (default: 10)') # numero di classi (default: 10, adatto a MNIST)\n","parser.add_argument('--gaussian_size', default=64, type=int,\n","                    help='gaussian size (default: 64)') # dimensione della gaussiana\n","parser.add_argument('--input_size', default=784, type=int,\n","                    help='input size (default: 784)') # dimensione dell'input (default: 28x28, adatto a MNIST)\n","\n","## Partition parameters\n","parser.add_argument('--train_proportion', default=1.0, type=float,\n","                    help='proportion of examples to consider for training only (default: 1.0)') # proporzione di esempi da considerare per l'addestramento, default 1.0 = tutti gli esempi\n","\n","## Gumbel parameters\n","#è tecnica che permette di approssimare la campionatura discreta in modo continuo\n","parser.add_argument('--init_temp', default=1.0, type=float,\n","                    help='Initial temperature used in gumbel-softmax (recommended 0.5-1.0, default:1.0)') # temperatura iniziale usata in gumbel-softmax\n","parser.add_argument('--decay_temp', default=1, type=int,\n","                    help='Set 1 to decay gumbel temperature at every epoch (default: 1)')# imposta 1 per decadere la temperatura gumbel ad ogni epoca\n","parser.add_argument('--hard_gumbel', default=0, type=int,\n","                    help='Set 1 to use the hard version of gumbel-softmax (default: 1)') # imposta 1 per usare la versione hard di gumbel-softmax\n","parser.add_argument('--min_temp', default=0.5, type=float,\n","                    help='Minimum temperature of gumbel-softmax after annealing (default: 0.5)' ) # temperatura minima di gumbel-softmax dopo il decadimento\n","parser.add_argument('--decay_temp_rate', default=0.013862944, type=float,\n","                    help='Temperature decay rate at every epoch (default: 0.013862944)') # tasso di decadimento della temperatura ad ogni epoca\n","\n","## Loss function parameters\n","parser.add_argument('--w_gauss', default=1, type=float,\n","                    help='weight of gaussian loss (default: 1)') # peso della perdita gaussiana\n","parser.add_argument('--w_categ', default=1, type=float,\n","                    help='weight of categorical loss (default: 1)') # peso della perdita categorica\n","parser.add_argument('--w_rec', default=1, type=float,\n","                    help='weight of reconstruction loss (default: 1)') # peso della perdita di ricostruzione\n","parser.add_argument('--rec_type', type=str, choices=['bce', 'mse'],\n","                    default='bce', help='desired reconstruction loss function (default: bce)') # tipo di funzione di perdita di ricostruzione desiderata\n","\n","## Others\n","parser.add_argument('--verbose', default=0, type=int,\n","                    help='print extra information at every epoch.(default: 0)') # stampa informazioni extra ad ogni epoca\n","parser.add_argument('--random_search_it', type=int, default=20,\n","                    help='iterations of random search (default: 20)') # iterazioni di ricerca casuale \n","\n","args = parser.parse_args()\n","# Questo script permette di configurare vari aspetti dell'addestramento di un modello di clustering basato su PyTorch, con opzioni per gestire il dataset, la GPU, \n","# i parametri di training, l'architettura e la funzione di perdita."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1714291160278,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"yJFbbw81kLWn","outputId":"30f8f9e9-1520-4b27-85d0-4bbb5f2e9705"},"outputs":[],"source":["args #contiene tutti i valori dei parametri e può essere utilizzato per configurare il comportamento del programma"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4166,"status":"ok","timestamp":1714291167359,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"Edat8F5tkCJu","outputId":"309d4c00-26a0-4a86-adb6-9e46fe656262"},"outputs":[],"source":["if args.dataset == \"mnist\":\n","  print(\"Loading mnist dataset...\")\n","  # Download or load downloaded MNIST dataset\n","  train_dataset = datasets.MNIST('./mnist', train=True, download=True, transform=transforms.ToTensor())\n","  test_dataset = datasets.MNIST('./mnist', train=False, transform=transforms.ToTensor())\n","# le immagini sono convertite in tensori"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1714291170485,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"Ox2eRDqakSjE","outputId":"4ad0cb3e-80a6-453a-c507-e3f787ada83a"},"outputs":[],"source":["image, label = train_dataset[0]\n","print(\"Image size:\", image.size())\n","print(\"Label:\", label) #indica la cifra rappresentata dall'immagine"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1714291172009,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"NUjoFEBlk30C"},"outputs":[],"source":["def show_image(image):\n","    plt.imshow(image.squeeze().numpy(), cmap='gray')\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259},"executionInfo":{"elapsed":1287,"status":"ok","timestamp":1714291174578,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"UbUN7iJxlVXa","outputId":"c9934b72-edb2-46ec-f703-7229eb2fbc8d"},"outputs":[],"source":["plt.figure(figsize=(15, 3))  # Set the figure size\n","\n","# Display the first 5 images\n","for i in range(5):\n","    image, label = train_dataset[i]\n","    plt.subplot(1, 5, i + 1)  # Create subplots for each image\n","    plt.imshow(image.squeeze().numpy(), cmap='gray')  # Convert tensor to numpy array and display\n","    plt.title('Label: %d' % label)  # Set the title\n","    plt.axis('off')  # Turn off axis\n","\n","plt.show()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":334,"status":"ok","timestamp":1714291178937,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"AemDGihhl06K"},"outputs":[],"source":["def partition_dataset(n, proportion=0.8):\n","  train_num = int(n * proportion) # numero di esempi da considerare per l'addestramento\n","  indices = np.random.permutation(n)\n","  train_indices, val_indices = indices[:train_num], indices[train_num:]\n","  return train_indices, val_indices\n","# Questa funzione divide gli indici del dataset in due sottoinsiemi: uno per il training e uno per la validazione.\n","\n","if args.train_proportion == 1.0: #tutto il dataset è usato per l'addestramento\n","  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False) # non si mescolano i dati perchè per il test l'ordine non è importante\n","  val_loader = test_loader # il test loader è usato come validation loader\n","else:\n","  train_indices, val_indices = partition_dataset(len(train_dataset), args.train_proportion)\n","  # Create data loaders for train, validation and test datasets\n","  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, sampler=SubsetRandomSampler(train_indices))\n","  val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size_val, sampler=SubsetRandomSampler(val_indices))\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":7852,"status":"ok","timestamp":1714291188869,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"tbQWK03cmGTk"},"outputs":[],"source":["for batch in train_loader:\n","    img, lab = batch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1714291190488,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"ok15ATTDmUw8","outputId":"7e81288b-05d8-4b46-bc56-4beab8d75f65"},"outputs":[],"source":["img.shape # 32 immagini per batch, 1 canale, 28x28 pixel\n","lab.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1714291208693,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"qvljI7iBmiu1","outputId":"2cf7b932-5827-4160-848e-3c75f792bd8e"},"outputs":[],"source":["## Calculate flatten size of each input data\n","args.input_size = np.prod(train_dataset[0][0].size())\n","print(args.input_size)"]},{"cell_type":"markdown","metadata":{"id":"I2YbzL-zmvpl"},"source":["## Network"]},{"cell_type":"markdown","metadata":{"id":"dFX57nFOm3Sk"},"source":["### reshape and flatten"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":292,"status":"ok","timestamp":1714291215222,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"V1psjWm-mw18"},"outputs":[],"source":["# Flatten layer (appiattisce un tensore multidimensionale in un tensore monodimensionale)\n","class Flatten(nn.Module):\n","  def forward(self, x):\n","    return x.view(x.size(0), -1)\n","# ogni immagine viene appiattita in un vettore di 784 elementi (28x28)\n","\n","# Reshape layer (converte un tensore in un'altra forma)\n","class Reshape(nn.Module):\n","  def __init__(self, outer_shape): # outer_shape è la nuova forma del tensore desiderata\n","    super(Reshape, self).__init__()\n","    self.outer_shape = outer_shape\n","  def forward(self, x):\n","    return x.view(x.size(0), *self.outer_shape)\n","#Reshape((1, 28, 28)) trasforma il tensore in un tensore di dimensione 1x28x28"]},{"cell_type":"markdown","metadata":{"id":"UOYjyn9MnCUO"},"source":["### Gumbel Softmax"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":226,"status":"ok","timestamp":1714291218406,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"jjDJUiQZnBdT"},"outputs":[],"source":["# Sample from the Gumbel-Softmax distribution and optionally discretize.\n","# The Gumbel-Softmax distribution is a continuous relaxation of the categorical distribution\n","class GumbelSoftmax(nn.Module):\n","\n","  def __init__(self, f_dim, c_dim):\n","    super(GumbelSoftmax, self).__init__()\n","    self.logits = nn.Linear(f_dim, c_dim)\n","    self.f_dim = f_dim\n","    self.c_dim = c_dim\n","    \"\"\"\n","    f_dim := feature dimension\n","    c_dim := number of categories\n","    logits := takes input of size f_dim and outputs c_dim dimensional logits\n","    \"\"\"\n","\n","  def sample_gumbel(self, shape, is_cuda=False, eps=1e-20):\n","    U = torch.rand(shape)\n","    if is_cuda:\n","      U = U.cuda()\n","    return -torch.log(-torch.log(U + eps) + eps)\n","  # campiona da una distribuzione di Gumbel\n","\n","  def gumbel_softmax_sample(self, logits, temperature):\n","    y = logits + self.sample_gumbel(logits.size(), logits.is_cuda)\n","    return F.softmax(y / temperature, dim=-1)\n","\n","  def gumbel_softmax(self, logits, temperature, hard=False):\n","    \"\"\"\n","    ST-gumple-softmax\n","    input: [*, n_class]\n","    return: flatten --> [*, n_class] an one-hot vector\n","    \"\"\"\n","    #categorical_dim = 10\n","    y = self.gumbel_softmax_sample(logits, temperature)\n","\n","    if not hard:\n","        return y\n","\n","    shape = y.size()\n","    _, ind = y.max(dim=-1)\n","    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n","    y_hard.scatter_(1, ind.view(-1, 1), 1)\n","    y_hard = y_hard.view(*shape)\n","    # Set gradients w.r.t. y_hard gradients w.r.t. y\n","    y_hard = (y_hard - y).detach() + y\n","    return y_hard\n","\n","  def forward(self, x, temperature=1.0, hard=False):\n","    logits = self.logits(x).view(-1, self.c_dim)\n","    prob = F.softmax(logits, dim=-1)\n","    y = self.gumbel_softmax(logits, temperature, hard)\n","    return logits, prob, y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325,"status":"ok","timestamp":1714292532670,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"ur8F-53_uCKq","outputId":"ac7d0308-35ad-461b-960e-d0d73461e9b8"},"outputs":[],"source":["# Define an instance of GumbelSoftmax module\n","f_dim = 10  # Feature dimension\n","c_dim = 3   # Number of categories\n","gumbel_softmax = GumbelSoftmax(f_dim, c_dim)\n","\n","# Generate random input tensor\n","batch_size = 2\n","input_tensor = torch.randn(batch_size, f_dim)\n","\n","# Test forward pass without hard sampling\n","logits, probabilities, sampled_values = gumbel_softmax(input_tensor)\n","print(\"Logits:\\n\", logits.detach().numpy()) # (batch_size, c_dim)\n","print(\"Probabilities:\\n\", probabilities.detach().numpy()) # Applying the softmax function to the logits, we obtain probabilities for each category\n","print(\"Sampled Values:\\n\", sampled_values.detach().numpy()) # Contains the sampled values obtained by applying Gumbel-Softmax sampling to the logits\n","print(\"\")\n","\n","# Test forward pass with hard sampling\n","logits_hard, probabilities_hard, sampled_values_hard = gumbel_softmax(input_tensor, hard=True)\n","print(\"Logits (Hard):\\n\", logits_hard.detach().numpy())\n","print(\"Probabilities (Hard):\\n\", probabilities_hard.detach().numpy())\n","print(\"Sampled Values (Hard):\\n\", sampled_values_hard.detach().numpy())\n","\n","#Logits e Probabilities sono uguali, mentre Sampled Values è diverso tra le versioni hard e soft"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":1327,"status":"ok","timestamp":1714292097030,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"S_t4Uk2OwUj5","outputId":"caf41831-dafb-4056-d97e-0a8acdff97b4"},"outputs":[],"source":["# NOTE: The temperature parameter controls the \"softness\" of the sampling process.\n","# Higher temperatures lead to softer distributions where the probabilities across categories are more evenly spread out,\n","# while lower temperatures result in harder distributions with sharper peaks\n","# Quando la temperatura è alta, le probabilità tra le categorie si uniformano, mentre quando è bassa ci si aspetta che una delle categorie domini le altre\n","\n","def sample_gumbel(shape, is_cuda=False, eps=1e-20):\n","    U = torch.rand(shape)\n","    if is_cuda:\n","      U = U.cuda()\n","    return -torch.log(-torch.log(U + eps) + eps)\n","\n","def gumbel_softmax_sample(logits, temperature):\n","    y = logits + sample_gumbel(logits.size(), logits.is_cuda)\n","    return F.softmax(y / temperature, dim=-1)\n","# Define logits\n","logits = torch.tensor([-1.0, 1.0, 2.0, 5.0])\n","\n","# Define temperature range\n","temperatures = torch.linspace(0.1, 5, 20)\n","\n","# Perform Gumbel-Softmax sampling for each temperature\n","samples = [gumbel_softmax_sample(logits, temp) for temp in temperatures]\n","\n","# Convert samples to numpy array for plotting\n","samples_np = torch.stack(samples).numpy()\n","\n","# Plot the sampled values for each temperature\n","plt.figure(figsize=(8, 5))\n","for i in range(logits.size(0)):\n","    plt.plot(temperatures.numpy(), samples_np[:, i], label=f'Category {i+1}')\n","\n","plt.xlabel('Temperature')\n","plt.ylabel('Probability')\n","plt.title('Variation of Gumbel-Softmax Sampling with Temperature')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","# categoria 4 domina le altre quando la temperatura è bassa, mentre tutte le categorie sono equiprobabili quando la temperatura è alta"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714292420746,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"-dnPVK-qnHRo"},"outputs":[],"source":["# Sample from a Gaussian distribution\n","class Gaussian(nn.Module):\n","  def __init__(self, in_dim, z_dim):\n","    super(Gaussian, self).__init__()\n","    self.mu = nn.Linear(in_dim, z_dim)\n","    self.var = nn.Linear(in_dim, z_dim)\n","\n","  def reparameterize(self, mu, var):\n","    std = torch.sqrt(var + 1e-10)\n","    noise = torch.randn_like(std)\n","    z = mu + noise * std\n","    return z\n","\n","  def forward(self, x):\n","    mu = self.mu(x)\n","    var = F.softplus(self.var(x))\n","    z = self.reparameterize(mu, var)\n","    return mu, var, z\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1714292492406,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"DYnnzCGryf62","outputId":"56bd7db6-368e-46a4-c904-2926e3a550cd"},"outputs":[],"source":["# Example usage\n","# Define input tensor x\n","x = torch.randn(1, 10)\n","\n","# Define the Gaussian module with input dimension 10 and latent space dimension 5\n","gaussian = Gaussian(10, 5)\n","\n","# Forward pass through the Gaussian module\n","mu, var, z = gaussian(x)\n","\n","# Print the mean, variance, and sampled latent variable\n","print(\"Mean:\", mu.detach().numpy())\n","print(\"Variance:\", var.detach().numpy())\n","print(\"Sampled latent variable z:\", z.detach().numpy())"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":915,"status":"ok","timestamp":1714294077003,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"R3LJeGDXnV7V"},"outputs":[],"source":["import torch.nn.init as init"]},{"cell_type":"markdown","metadata":{"id":"xoDpcTn-nZ2N"},"source":["### Inference Network"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1714294818300,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"CkVuONHdnwkK"},"outputs":[],"source":["# Inference Network\n","class InferenceNet(nn.Module):\n","  def __init__(self, x_dim, z_dim, y_dim):\n","    super(InferenceNet, self).__init__()\n","    # ci sono due reti neurali: una per q(y|x) e una per q(z|y,x)\n","    # q(y|x)\n","    self.inference_qyx = torch.nn.ModuleList([\n","        nn.Linear(x_dim, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        GumbelSoftmax(512, y_dim)\n","    ])\n","\n","    # q(z|y,x)\n","    self.inference_qzyx = torch.nn.ModuleList([\n","        nn.Linear(x_dim + y_dim, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        Gaussian(512, z_dim)\n","    ])\n","\n","  # q(y|x)\n","  def qyx(self, x, temperature, hard):\n","    num_layers = len(self.inference_qyx)\n","    for i, layer in enumerate(self.inference_qyx):\n","      if i == num_layers - 1:\n","        #last layer is gumbel softmax\n","        x = layer(x, temperature, hard)\n","      else:\n","        x = layer(x)\n","    return x\n","  # funzione per calcolare q(y|x)\n","\n","  # q(z|x,y)\n","  def qzxy(self, x, y):\n","    concat = torch.cat((x, y), dim=1) # combina l'input di x e y\n","    for layer in self.inference_qzyx:\n","      concat = layer(concat)\n","    return concat\n","\n","  def forward(self, x, temperature=1.0, hard=0):\n","    #x = Flatten(x)\n","\n","    # q(y|x)\n","    logits, prob, y = self.qyx(x, temperature, hard)\n","\n","    # q(z|x,y)\n","    mu, var, z = self.qzxy(x, y)\n","\n","    output = {'mean': mu, 'var': var, 'gaussian': z,\n","              'logits': logits, 'prob_cat': prob, 'categorical': y}\n","    return output\n","# in input prende un immagine x\n","# la rete usa il metodo qyx  inferire la variabile latente discreta y data l'immagine di input x. Questo viene fatto approssimando la distribuzione categoriale con Gumbel-Softmax.\n","# La rete usa il metodo qzxy per inferire la variabile latente continua z data l'immagine x e la variabile latente discreta y.\n","# in output restituisce la media mu, la varianza var e il campione z della variabile latente continua z, i logit, la probabilità e il campione y della variabile latente discreta y."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1714294936170,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"BHuplNI48B8j","outputId":"2b21843e-4083-4958-fede-f48cd06b6d4f"},"outputs":[],"source":["show_image(img[3])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1714295065995,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"u87gz4JL8ZAl","outputId":"83acf3c6-a3b7-4caf-bbee-944d7dc4fdcf"},"outputs":[],"source":["image = img[0].unsqueeze(0) # Batch = 1\n","print(image.shape)\n","\n","flatten_img = image.view(image.size(0), -1)\n","print(flatten_img.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1714295226572,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"tDd0fIaR7wqh","outputId":"5c67184d-348d-4d06-d546-30b7c2e2aad4"},"outputs":[],"source":["# Example usage\n","# Assuming x_dim = 784, z_dim = 8, y_dim = 10\n","x_dim = 784\n","z_dim = 8\n","y_dim = 10\n","\n","# Create an instance of InferenceNet\n","inference_net = InferenceNet(x_dim, z_dim, y_dim)\n","\n","# Perform forward pass\n","output = inference_net(flatten_img)\n","\n","# Print the outputprint(\"Output:\")\n","for key, value in output.items():\n","    print(f\"{key}: {value}\")\n","print(\"\")\n","print(output['categorical'].sum())"]},{"cell_type":"markdown","metadata":{"id":"D51xLa_-nyYf"},"source":["### Generative Network"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":292,"status":"ok","timestamp":1714295777028,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"Oac_vz5Zn3Og"},"outputs":[],"source":["# Generative Network\n","class GenerativeNet(nn.Module):\n","  def __init__(self, x_dim, z_dim, y_dim):\n","    super(GenerativeNet, self).__init__()\n","\n","    # p(z|y)\n","    self.y_mu = nn.Linear(y_dim, z_dim)\n","    self.y_var = nn.Linear(y_dim, z_dim)\n","\n","    # p(x|z) genera x dato z\n","    self.generative_pxz = torch.nn.ModuleList([\n","        nn.Linear(z_dim, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, x_dim),\n","        torch.nn.Sigmoid() # garantisce che l'output sia compreso tra 0 e 1\n","    ])\n","\n","  # p(z|y)\n","  def pzy(self, y):\n","    y_mu = self.y_mu(y)\n","    y_var = F.softplus(self.y_var(y)) # garantisce che la varianza sia sempre positiva\n","    return y_mu, y_var\n","\n","  # p(x|z)\n","  def pxz(self, z):\n","    for layer in self.generative_pxz:\n","      z = layer(z)\n","    return z\n","\n","  def forward(self, z, y):\n","    # p(z|y)\n","    y_mu, y_var = self.pzy(y)\n","\n","    # p(x|z)\n","    x_rec = self.pxz(z)\n","\n","    output = {'y_mean': y_mu, 'y_var': y_var, 'x_rec': x_rec}\n","    return output\n","  \n","  # in input la classe prende una variabile latente z e una variabile categorica y\n","  # la rete usa il metodo pzy per calcolare la media e la varianza della distribuzione gaussiana di z data y\n","  # e il metodo pxz per generare l'immagine x dato il campione z\n","  # in output la rete restituisce la media e la varianza delle variabili latenti y e l'immagine generata x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1714296513348,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"P0G-_A81-aG1","outputId":"686b2231-f2e2-4d00-d8a8-f32a4ae278f1"},"outputs":[],"source":["# Example usage\n","# Assuming x_dim = 784, z_dim = 8, y_dim = 10\n","x_dim = 784\n","z_dim = 8\n","y_dim = 10\n","\n","# Create an instance of GenerativeNet\n","generative_net = GenerativeNet(x_dim, z_dim, y_dim)\n","\n","# Perform forward pass\n","output_gen = generative_net(output['gaussian'], output['categorical'])\n","\n","# Print the output\n","for key, value in list(output_gen.items())[:-1]:\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1714296086754,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"0PxFgnuVAfF1","outputId":"4bcd03dd-c7b6-4ef4-df6c-d15577c76edc"},"outputs":[],"source":["show_image(output_gen['x_rec'].view(1,28,28).detach())"]},{"cell_type":"markdown","metadata":{"id":"V74fIux_n-JM"},"source":["### GM VAE"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1714296383577,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"5dfi5F_5n_sI"},"outputs":[],"source":["# GMVAE Network\n","class GMVAENet(nn.Module):\n","  def __init__(self, x_dim, z_dim, y_dim):\n","    super(GMVAENet, self).__init__()\n","\n","    self.inference = InferenceNet(x_dim, z_dim, y_dim) # usato per inferire le variabili z e y da un'immagine x\n","    self.generative = GenerativeNet(x_dim, z_dim, y_dim) # usato per generare un'immagine x data una variabile latente z e una variabile categorica y\n","\n","    # weight initialization\n","    for m in self.modules():\n","      if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d: # inizializza i pesi dei layer lineari e convoluzionali\n","        torch.nn.init.xavier_normal_(m.weight)\n","        if m.bias.data is not None: # se ci sono bias nei layer venogono inizializzati a 0\n","          init.constant_(m.bias, 0)\n","\n","  def forward(self, x, temperature=1.0, hard=0):\n","    x = x.view(x.size(0), -1) # appiattisce l'immagine in un vettore\n","    out_inf = self.inference(x, temperature, hard) # inferisce le variabili latenti z e categoriali y\n","    z, y = out_inf['gaussian'], out_inf['categorical'] # estrae z e y\n","    out_gen = self.generative(z, y) # genera un'immagine x data z e y\n","\n","    # merge output\n","    output = out_inf\n","    for key, value in out_gen.items():\n","      output[key] = value\n","    return output\n","  # in input la rete prende un'immagine x\n","  # x viene appiattiata e passata alla rete di inferenza per inferire le variabili latenti z e categoriali y\n","  # z e y vengono passati alla rete generativa per generare un'immagine x\n","  # in output la rete restituisce le variabili latenti z e y e l'immagine generata x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":276,"status":"ok","timestamp":1714296464355,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"uF5rvb-QBjKR","outputId":"afa28eb8-20e6-404a-ea01-4f7c4281f5a0"},"outputs":[],"source":["# Example usage\n","# Assuming x_dim = 784 (28x28), z_dim = 8, y_dim = 10\n","x_dim = 784\n","z_dim = 8\n","y_dim = 10\n","\n","# Create an instance of GMVAENet\n","gmvae_net = GMVAENet(x_dim, z_dim, y_dim)\n","\n","# Perform forward pass\n","output = gmvae_net(image)\n","\n","# Print the output\n","for key, value in list(output.items())[:-1]:\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1714296484835,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-120},"id":"_0IVqIDjCFg8","outputId":"33c16f7d-ead5-48ab-9907-c5f7c174a62c"},"outputs":[],"source":["show_image(output['x_rec'].view(1,28,28).detach())"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"FozGBmy-oPUN"},"outputs":[],"source":["import math\n","import torch\n","import numpy as np\n","from torch import nn"]},{"cell_type":"markdown","metadata":{"id":"jpz-RoxWoQFX"},"source":["## Losses"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"hBaFwdyEoSDM"},"outputs":[],"source":["# le loss functions misurano quanto bene il modello è in grado di generare i dati di input\n","class LossFunctions:\n","    eps = 1e-8\n","\n","    def mean_squared_error(self, real, predictions):\n","      \"\"\"Mean Squared Error between the true and predicted outputs\n","         loss = (1/n)*Σ(real - predicted)^2\n","\n","      Args:\n","          real: (array) corresponding array containing the true labels\n","          predictions: (array) corresponding array containing the predicted labels\n","\n","      Returns:\n","          output: (array/float) depending on average parameters the result will be the mean\n","                                of all the sample losses or an array with the losses per sample\n","      \"\"\"\n","      loss = (real - predictions).pow(2)\n","      return loss.sum(-1).mean()\n","\n","\n","    def reconstruction_loss(self, real, predicted, rec_type='mse' ):\n","      \"\"\"Reconstruction loss between the true and predicted outputs\n","         mse = (1/n)*Σ(real - predicted)^2\n","         bce = (1/n) * -Σ(real*log(predicted) + (1 - real)*log(1 - predicted))\n","\n","      Args:\n","          real: (array) corresponding array containing the true labels\n","          predictions: (array) corresponding array containing the predicted labels\n","\n","      Returns:\n","          output: (array/float) depending on average parameters the result will be the mean\n","                                of all the sample losses or an array with the losses per sample\n","      \"\"\"\n","      if rec_type == 'mse':\n","        loss = (real - predicted).pow(2)\n","      elif rec_type == 'bce':\n","        loss = F.binary_cross_entropy(predicted, real, reduction='none')\n","      else:\n","        raise \"invalid loss function... try bce or mse...\"\n","      return loss.sum(-1).mean()\n","\n","\n","    def log_normal(self, x, mu, var):\n","      \"\"\"Logarithm of normal distribution with mean=mu and variance=var\n","         log(x|μ, σ^2) = loss = -0.5 * Σ log(2π) + log(σ^2) + ((x - μ)/σ)^2\n","\n","      Args:\n","         x: (array) corresponding array containing the input\n","         mu: (array) corresponding array containing the mean\n","         var: (array) corresponding array containing the variance\n","\n","      Returns:\n","         output: (array/float) depending on average parameters the result will be the mean\n","                                of all the sample losses or an array with the losses per sample\n","      \"\"\"\n","      if self.eps > 0.0:\n","        var = var + self.eps\n","      return -0.5 * torch.sum(\n","        np.log(2.0 * np.pi) + torch.log(var) + torch.pow(x - mu, 2) / var, dim=-1)\n","\n","\n","    def gaussian_loss(self, z, z_mu, z_var, z_mu_prior, z_var_prior):\n","      \"\"\"Variational loss when using labeled data without considering reconstruction loss\n","         loss = log q(z|x,y) - log p(z) - log p(y)\n","\n","      Args:\n","         z: (array) array containing the gaussian latent variable\n","         z_mu: (array) array containing the mean of the inference model\n","         z_var: (array) array containing the variance of the inference model\n","         z_mu_prior: (array) array containing the prior mean of the generative model\n","         z_var_prior: (array) array containing the prior variance of the generative mode\n","\n","      Returns:\n","         output: (array/float) depending on average parameters the result will be the mean\n","                                of all the sample losses or an array with the losses per sample\n","      \"\"\"\n","      loss = self.log_normal(z, z_mu, z_var) - self.log_normal(z, z_mu_prior, z_var_prior)\n","      return loss.mean()\n","\n","\n","    def entropy(self, logits, targets):\n","      \"\"\"Entropy loss\n","          loss = (1/n) * -Σ targets*log(predicted)\n","\n","      Args:\n","          logits: (array) corresponding array containing the logits of the categorical variable\n","          real: (array) corresponding array containing the true labels\n","\n","      Returns:\n","          output: (array/float) depending on average parameters the result will be the mean\n","                                of all the sample losses or an array with the losses per sample\n","      \"\"\"\n","      log_q = F.log_softmax(logits, dim=-1)\n","      return -torch.mean(torch.sum(targets * log_q, dim=-1))"]},{"cell_type":"markdown","metadata":{"id":"tL1wGKL6pIXW"},"source":["## Metrics"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"sJXUK91fpJ1u"},"outputs":[],"source":["import numpy as np\n","from scipy.optimize import linear_sum_assignment\n","from sklearn.metrics.cluster import normalized_mutual_info_score\n","# valuta le prestazioni del modello di clustering\n","class Metrics:\n","\n","  # Code taken from the work\n","  # VaDE (Variational Deep Embedding:A Generative Approach to Clustering)\n","  def cluster_acc(self, Y_pred, Y):\n","    Y_pred, Y = np.array(Y_pred), np.array(Y)\n","    assert Y_pred.size == Y.size\n","    D = max(Y_pred.max(), Y.max())+1\n","    w = np.zeros((D,D), dtype=np.int64)\n","    for i in range(Y_pred.size):\n","      w[Y_pred[i], Y[i]] += 1\n","    row, col = linear_sum_assignment(w.max()-w)\n","    return sum([w[row[i],col[i]] for i in range(row.shape[0])]) * 1.0/Y_pred.size\n","  # confronta le etichette predette con quelle vere e restituisce l'accuratezza del clustering\n","\n","  def nmi(self, Y_pred, Y):\n","    Y_pred, Y = np.array(Y_pred), np.array(Y)\n","    assert Y_pred.size == Y.size\n","    return normalized_mutual_info_score(Y_pred, Y, average_method='arithmetic')\n","  # calcola l'informazione mutua normalizzata tra le etichette predette e quelle vere\n","  # 1 indica che le etichette predette e vere sono identiche, 0 indica che le etichette sono indipendenti"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"1fpL8Ol9o6GL"},"outputs":[],"source":["from torch import nn, optim"]},{"cell_type":"markdown","metadata":{"id":"E18YW6uNo-Ub"},"source":["## Class GMVAE"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"hZoeM24WpAim"},"outputs":[],"source":["# \n","class GMVAE:\n","\n","  def __init__(self, args):\n","    self.num_epochs = args.epochs\n","    self.cuda = args.cuda\n","    self.verbose = args.verbose\n","\n","    self.batch_size = args.batch_size\n","    self.batch_size_val = args.batch_size_val\n","    self.learning_rate = args.learning_rate\n","    self.decay_epoch = args.decay_epoch\n","    self.lr_decay = args.lr_decay\n","    self.w_cat = args.w_categ\n","    self.w_gauss = args.w_gauss\n","    self.w_rec = args.w_rec\n","    self.rec_type = args.rec_type\n","\n","    self.num_classes = args.num_classes\n","    self.gaussian_size = args.gaussian_size\n","    self.input_size = args.input_size\n","\n","    # gumbel\n","    self.init_temp = args.init_temp\n","    self.decay_temp = args.decay_temp\n","    self.hard_gumbel = args.hard_gumbel\n","    self.min_temp = args.min_temp\n","    self.decay_temp_rate = args.decay_temp_rate\n","    self.gumbel_temp = self.init_temp\n","\n","    self.network = GMVAENet(self.input_size, self.gaussian_size, self.num_classes)\n","    self.losses = LossFunctions()\n","    self.metrics = Metrics()\n","\n","    if self.cuda:\n","      self.network = self.network.cuda()\n","\n","\n","  def unlabeled_loss(self, data, out_net):\n","    \"\"\"Method defining the loss functions derived from the variational lower bound\n","    Args:\n","        data: (array) corresponding array containing the input data\n","        out_net: (dict) contains the graph operations or nodes of the network output\n","\n","    Returns:\n","        loss_dic: (dict) contains the values of each loss function and predictions\n","    \"\"\"\n","    # obtain network variables\n","    z, data_recon = out_net['gaussian'], out_net['x_rec']\n","    logits, prob_cat = out_net['logits'], out_net['prob_cat']\n","    y_mu, y_var = out_net['y_mean'], out_net['y_var']\n","    mu, var = out_net['mean'], out_net['var']\n","\n","    # reconstruction loss\n","    loss_rec = self.losses.reconstruction_loss(data, data_recon, self.rec_type)\n","\n","    # gaussian loss\n","    loss_gauss = self.losses.gaussian_loss(z, mu, var, y_mu, y_var)\n","\n","    # categorical loss\n","    loss_cat = -self.losses.entropy(logits, prob_cat) - np.log(0.1)\n","\n","    # total loss\n","    loss_total = self.w_rec * loss_rec + self.w_gauss * loss_gauss + self.w_cat * loss_cat\n","\n","    # obtain predictions\n","    _, predicted_labels = torch.max(logits, dim=1)\n","\n","    loss_dic = {'total': loss_total,\n","                'predicted_labels': predicted_labels,\n","                'reconstruction': loss_rec,\n","                'gaussian': loss_gauss,\n","                'categorical': loss_cat}\n","    return loss_dic\n","\n","\n","  def train_epoch(self, optimizer, data_loader):\n","    \"\"\"Train the model for one epoch\n","\n","    Args:\n","        optimizer: (Optim) optimizer to use in backpropagation\n","        data_loader: (DataLoader) corresponding loader containing the training data\n","\n","    Returns:\n","        average of all loss values, accuracy, nmi\n","    \"\"\"\n","    self.network.train()\n","    total_loss = 0.\n","    recon_loss = 0.\n","    cat_loss = 0.\n","    gauss_loss = 0.\n","\n","    accuracy = 0.\n","    nmi = 0.\n","    num_batches = 0.\n","\n","    true_labels_list = []\n","    predicted_labels_list = []\n","\n","    # iterate over the dataset\n","    for (data, labels) in data_loader:\n","      if self.cuda == 1:\n","        data = data.cuda()\n","\n","      optimizer.zero_grad()\n","\n","      # flatten data\n","      data = data.view(data.size(0), -1)\n","\n","      # forward call\n","      out_net = self.network(data, self.gumbel_temp, self.hard_gumbel)\n","      unlab_loss_dic = self.unlabeled_loss(data, out_net)\n","      total = unlab_loss_dic['total']\n","\n","      # accumulate values\n","      total_loss += total.item()\n","      recon_loss += unlab_loss_dic['reconstruction'].item()\n","      gauss_loss += unlab_loss_dic['gaussian'].item()\n","      cat_loss += unlab_loss_dic['categorical'].item()\n","\n","      # perform backpropagation\n","      total.backward()\n","      optimizer.step()\n","\n","      # save predicted and true labels\n","      predicted = unlab_loss_dic['predicted_labels']\n","      true_labels_list.append(labels)\n","      predicted_labels_list.append(predicted)\n","\n","      num_batches += 1.\n","\n","    # average per batch\n","    total_loss /= num_batches\n","    recon_loss /= num_batches\n","    gauss_loss /= num_batches\n","    cat_loss /= num_batches\n","\n","    # concat all true and predicted labels\n","    true_labels = torch.cat(true_labels_list, dim=0).cpu().numpy()\n","    predicted_labels = torch.cat(predicted_labels_list, dim=0).cpu().numpy()\n","\n","    # compute metrics\n","    accuracy = 100.0 * self.metrics.cluster_acc(predicted_labels, true_labels)\n","    nmi = 100.0 * self.metrics.nmi(predicted_labels, true_labels)\n","\n","    return total_loss, recon_loss, gauss_loss, cat_loss, accuracy, nmi\n","\n","\n","  # valuta il modello su un dataset di test o validazione\n","  def test(self, data_loader, return_loss=False):\n","    \"\"\"Test the model with new data\n","\n","    Args:\n","        data_loader: (DataLoader) corresponding loader containing the test/validation data\n","        return_loss: (boolean) whether to return the average loss values\n","\n","    Return:\n","        accuracy and nmi for the given test data\n","\n","    \"\"\"\n","    self.network.eval()\n","    total_loss = 0.\n","    recon_loss = 0.\n","    cat_loss = 0.\n","    gauss_loss = 0.\n","\n","    accuracy = 0.\n","    nmi = 0.\n","    num_batches = 0.\n","\n","    true_labels_list = []\n","    predicted_labels_list = []\n","\n","    with torch.no_grad(): # disabilita il calcolo del gradiente\n","      for data, labels in data_loader:\n","        if self.cuda == 1:\n","          data = data.cuda() # data viene spostato sulla GPU\n","\n","        # flatten data\n","        data = data.view(data.size(0), -1)\n","\n","        # forward call\n","        out_net = self.network(data, self.gumbel_temp, self.hard_gumbel)\n","        unlab_loss_dic = self.unlabeled_loss(data, out_net)\n","\n","        # accumulate values\n","        total_loss += unlab_loss_dic['total'].item()\n","        recon_loss += unlab_loss_dic['reconstruction'].item()\n","        gauss_loss += unlab_loss_dic['gaussian'].item()\n","        cat_loss += unlab_loss_dic['categorical'].item()\n","\n","        # save predicted and true labels\n","        predicted = unlab_loss_dic['predicted_labels']\n","        true_labels_list.append(labels)\n","        predicted_labels_list.append(predicted)\n","\n","        num_batches += 1.\n","\n","    # average per batch\n","    if return_loss:\n","      total_loss /= num_batches\n","      recon_loss /= num_batches\n","      gauss_loss /= num_batches\n","      cat_loss /= num_batches\n","\n","    # concat all true and predicted labels\n","    true_labels = torch.cat(true_labels_list, dim=0).cpu().numpy()\n","    predicted_labels = torch.cat(predicted_labels_list, dim=0).cpu().numpy()\n","\n","    # compute metrics\n","    accuracy = 100.0 * self.metrics.cluster_acc(predicted_labels, true_labels)\n","    nmi = 100.0 * self.metrics.nmi(predicted_labels, true_labels)\n","\n","    if return_loss:\n","      return total_loss, recon_loss, gauss_loss, cat_loss, accuracy, nmi\n","    else:\n","      return accuracy, nmi\n","\n","\n","  def train(self, train_loader, val_loader):\n","    \"\"\"Train the model\n","\n","    Args:\n","        train_loader: (DataLoader) corresponding loader containing the training data\n","        val_loader: (DataLoader) corresponding loader containing the validation data\n","\n","    Returns:\n","        output: (dict) contains the history of train/val loss\n","    \"\"\"\n","    optimizer = optim.Adam(self.network.parameters(), lr=self.learning_rate)\n","    train_history_acc, val_history_acc = [], []\n","    train_history_nmi, val_history_nmi = [], []\n","\n","    for epoch in range(1, self.num_epochs + 1):\n","      train_loss, train_rec, train_gauss, train_cat, train_acc, train_nmi = self.train_epoch(optimizer, train_loader)\n","      val_loss, val_rec, val_gauss, val_cat, val_acc, val_nmi = self.test(val_loader, True)\n","\n","      # if verbose then print specific information about training\n","      if self.verbose == 1:\n","        print(\"(Epoch %d / %d)\" % (epoch, self.num_epochs) )\n","        print(\"Train - REC: %.5lf;  Gauss: %.5lf;  Cat: %.5lf;\" % \\\n","              (train_rec, train_gauss, train_cat))\n","        print(\"Valid - REC: %.5lf;  Gauss: %.5lf;  Cat: %.5lf;\" % \\\n","              (val_rec, val_gauss, val_cat))\n","        print(\"Accuracy=Train: %.5lf; Val: %.5lf   NMI=Train: %.5lf; Val: %.5lf   Total Loss=Train: %.5lf; Val: %.5lf\" % \\\n","              (train_acc, val_acc, train_nmi, val_nmi, train_loss, val_loss))\n","      else:\n","        print('(Epoch %d / %d) Train_Loss: %.3lf; Val_Loss: %.3lf   Train_ACC: %.3lf; Val_ACC: %.3lf   Train_NMI: %.3lf; Val_NMI: %.3lf' % \\\n","              (epoch, self.num_epochs, train_loss, val_loss, train_acc, val_acc, train_nmi, val_nmi))\n","\n","      # decay gumbel temperature\n","      if self.decay_temp == 1:\n","        self.gumbel_temp = np.maximum(self.init_temp * np.exp(-self.decay_temp_rate * epoch), self.min_temp)\n","        if self.verbose == 1:\n","          print(\"Gumbel Temperature: %.3lf\" % self.gumbel_temp)\n","\n","      train_history_acc.append(train_acc)\n","      val_history_acc.append(val_acc)\n","      train_history_nmi.append(train_nmi)\n","      val_history_nmi.append(val_nmi)\n","    return {'train_history_nmi' : train_history_nmi, 'val_history_nmi': val_history_nmi,\n","            'train_history_acc': train_history_acc, 'val_history_acc': val_history_acc}\n","\n","\n","  def latent_features(self, data_loader, return_labels=False):\n","    \"\"\"Obtain latent features learnt by the model\n","\n","    Args:\n","        data_loader: (DataLoader) loader containing the data\n","        return_labels: (boolean) whether to return true labels or not\n","\n","    Returns:\n","       features: (array) array containing the features from the data\n","    \"\"\"\n","    self.network.eval()\n","    N = len(data_loader.dataset)\n","    features = np.zeros((N, self.gaussian_size))\n","    if return_labels:\n","      true_labels = np.zeros(N, dtype=np.int64)\n","    start_ind = 0\n","    with torch.no_grad():\n","      for (data, labels) in data_loader:\n","        if self.cuda == 1:\n","          data = data.cuda()\n","        # flatten data\n","        data = data.view(data.size(0), -1)\n","        out = self.network.inference(data, self.gumbel_temp, self.hard_gumbel)\n","        latent_feat = out['mean']\n","        end_ind = min(start_ind + data.size(0), N+1)\n","\n","        # return true labels\n","        if return_labels:\n","          true_labels[start_ind:end_ind] = labels.cpu().numpy()\n","        features[start_ind:end_ind] = latent_feat.cpu().detach().numpy()\n","        start_ind += data.size(0)\n","    if return_labels:\n","      return features, true_labels\n","    return features\n","\n","\n","  def reconstruct_data(self, data_loader, sample_size=-1):\n","    \"\"\"Reconstruct Data\n","\n","    Args:\n","        data_loader: (DataLoader) loader containing the data\n","        sample_size: (int) size of random data to consider from data_loader\n","\n","    Returns:\n","        reconstructed: (array) array containing the reconstructed data\n","    \"\"\"\n","    self.network.eval()\n","\n","    # sample random data from loader\n","    indices = np.random.randint(0, len(data_loader.dataset), size=sample_size)\n","    test_random_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=sample_size, sampler=SubsetRandomSampler(indices))\n","\n","    # obtain values\n","    it = iter(test_random_loader)\n","    test_batch_data, _ = next(it) #MODIFIED\n","    original = test_batch_data.data.numpy()\n","    if self.cuda:\n","      test_batch_data = test_batch_data.cuda()\n","\n","    # obtain reconstructed data\n","    out = self.network(test_batch_data, self.gumbel_temp, self.hard_gumbel)\n","    reconstructed = out['x_rec']\n","    return original, reconstructed.data.cpu().numpy()\n","\n","\n","  def plot_latent_space(self, data_loader, save=False):\n","    \"\"\"Plot the latent space learnt by the model\n","\n","    Args:\n","        data: (array) corresponding array containing the data\n","        labels: (array) corresponding array containing the labels\n","        save: (bool) whether to save the latent space plot\n","\n","    Returns:\n","        fig: (figure) plot of the latent space\n","    \"\"\"\n","    # obtain the latent features\n","    features = self.latent_features(data_loader)\n","\n","    # plot only the first 2 dimensions\n","    fig = plt.figure(figsize=(8, 6))\n","    plt.scatter(features[:, 0], features[:, 1], c=labels, marker='o',\n","            edgecolor='none', cmap=plt.cm.get_cmap('jet', 10), s = 10)\n","    plt.colorbar()\n","    if(save):\n","        fig.savefig('latent_space.png')\n","    return fig\n","\n","\n","  def random_generation(self, num_elements=1):\n","    \"\"\"Random generation for each category\n","\n","    Args:\n","        num_elements: (int) number of elements to generate\n","\n","    Returns:\n","        generated data according to num_elements\n","    \"\"\"\n","    # categories for each element\n","    arr = np.array([])\n","    for i in range(self.num_classes):\n","      arr = np.hstack([arr,np.ones(num_elements) * i] )\n","    indices = arr.astype(int).tolist()\n","\n","    categorical = F.one_hot(torch.tensor(indices), self.num_classes).float()\n","\n","    if self.cuda:\n","      categorical = categorical.cuda() # il tensore categorical viene spostato su GPU\n","\n","    # infer the gaussian distribution according to the category\n","    mean, var = self.network.generative.pzy(categorical)\n","\n","    # gaussian random sample by using the mean and variance\n","    noise = torch.randn_like(var)\n","    std = torch.sqrt(var)\n","    gaussian = mean + noise * std\n","\n","    # generate new samples with the given gaussian\n","    generated = self.network.generative.pxz(gaussian)\n","\n","    return generated.cpu().detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OTALMVHqlpG"},"outputs":[],"source":["gmvae = GMVAE(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3951810,"status":"ok","timestamp":1708340520347,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-60},"id":"1NsJVGvBqjwZ","outputId":"b774fdbf-235a-40e6-c568-c4b01d53b980"},"outputs":[],"source":["## Training Phase\n","history_loss = gmvae.train(train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1704,"status":"ok","timestamp":1708340524027,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-60},"id":"u7HUPHYqsKUT","outputId":"99a777ad-6f95-4212-9106-2946517d695b"},"outputs":[],"source":["accuracy, nmi = gmvae.test(test_loader)\n","\n","print(\"Testing phase...\")\n","print(\"Accuracy: %.5lf,  NMI: %.5lf\" % (accuracy, nmi) )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"elapsed":1679,"status":"ok","timestamp":1708340530207,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-60},"id":"4sou1TbAsNL5","outputId":"45550d74-b6c8-4f43-cb19-d4f3a722c074"},"outputs":[],"source":["def display_reconstructed(original, reconstructed, n=10):\n","  plt.figure(figsize=[18,2])\n","  for i in range(n):\n","    plt.subplot(2, n, i + 1)\n","    plt.imshow(original[i].reshape(28, 28))\n","    plt.gray()\n","    plt.axis('off')\n","\n","    if reconstructed is not None:\n","      plt.subplot(2, n, i + n + 1)\n","      plt.imshow(reconstructed[i].reshape(28, 28))\n","      plt.gray()\n","      plt.axis('off')\n","  plt.show()\n","# Display original and reconstructed images\n","\n","original, reconstructed = gmvae.reconstruct_data(test_loader, 15)\n","display_reconstructed(original, reconstructed, 15)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":807},"executionInfo":{"elapsed":3771,"status":"ok","timestamp":1708340539799,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-60},"id":"W6HMjMbgsNyX","outputId":"0dd662a7-d920-43f4-bac0-90cee19054bf"},"outputs":[],"source":["def display_random_generation(generated, num_classes, n=10):\n","  plt.figure(figsize=[10,10])\n","  for c in range(num_classes):\n","    for i in range(n):\n","      plt.subplot(num_classes, n, (c * n) + i + 1)\n","      plt.imshow(generated[(c * n) + i].reshape(28, 28))\n","      plt.gray()\n","      plt.axis('off')\n","  plt.show()\n","\n","elem_per_category = 10\n","generated = gmvae.random_generation(elem_per_category)\n","display_random_generation(generated, args.num_classes, elem_per_category)\n","# per ogni categoria vengono generati 10 elementi"]},{"cell_type":"markdown","metadata":{"id":"-SbXR7FkhIcq"},"source":["## Visualization of the feature latent space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTkEBA9JhQ2C"},"outputs":[],"source":["# get feature representations\n","test_features, test_labels = gmvae.latent_features(test_loader, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNmsz5rahZAY"},"outputs":[],"source":["# import TSNE from scikit-learn library\n","from sklearn.manifold import TSNE\n","\n","# reduce dimensionality to 2D, we consider a subset of data because TSNE\n","# is a slow algorithm\n","tsne_features = TSNE(n_components=2).fit_transform(test_features[:1000,])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":584},"executionInfo":{"elapsed":1283,"status":"ok","timestamp":1708340578288,"user":{"displayName":"Luca Caldera","userId":"08170332462513010463"},"user_tz":-60},"id":"wyTtDdwyha-L","outputId":"ebf84ed4-0dd3-4e35-b0a8-0afb9aa8e029"},"outputs":[],"source":["fig = plt.figure(figsize=(10, 6))\n","\n","plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=test_labels[:tsne_features.shape[0]], marker='o',\n","            edgecolor='none', cmap=plt.cm.get_cmap('jet', 10), s = 10)\n","plt.grid(False)\n","plt.axis('off')\n","plt.colorbar()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN83wFEjMlj8gNDAgwvWQnx","gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
